\section{Introduction}
\label{sec:introduction}

In the era of the telephone voice traffic dominated physical
telecommunication lines. With the birth of the Internet, and its
subsequent adoption as a key means of communication, data traffic has
taken over (though some of this data traffic is actually re-badged
voice traffic using Voice over IP).  With the advent of new
applications such as video streaming, and the rapid growth of data
traffic from mobile devices, we are witnessing a global data
explosion.

Given the ever increasing importance of the Internet, knowledge of its
traffic has become increasingly critical.  The Internet, however, is
just an all-encompassing term to describe the vast global collection
of networks, and so it largely falls on individual network providers
to determine their own traffic.  This knowledge is vital for continued
operations because it allows network operators to perform important
tasks such as providing enough network capacity to carry the current
traffic, as well as to predict and prepare for future trends. Traffic
data is also important in network maintenance, which is necessary if
services and content are to be provided to customers with minimal
interruption.

The focus of this chapter is on the {\em traffic matrix}, which, in a
nutshell, is an abstract representation of the traffic volume flowing
between sets of source and destination pairs. Each element in the
matrix denotes the amount of traffic between a source and destination
pair.  There are many variants: depending on the network layer under
study, sources and destinations could be routers or even whole
networks. And ``Amount'' is generally measured in the number of bytes
or packets, but could refer to other quantities such as connections.

Traffic matrices, as will be clearer below, are utilised for a
variety of network engineering goals, such as prediction of future
traffic trends, network optimisation, protocol design and anomaly
detection.  Considering the widespread use of these matrices, the first
objective of this chapter is to provide an entry level for graduate
students and new researchers to current research on traffic
matrices. To that end, the material is organised in a tutorial-like
fashion, so as to ensure key concepts can be easily understood. 

% In
% addition, it is hoped this chapter would be a definitive reference for
% researchers and practitioners directly involved in using traffic
% matrices for network engineering applications.


\subsection{Motivation}

Why study Internet traffic matrices? Simply because their implications
for network operators are vast. If the traffic matrix of a network is
exactly known, then, armed with topology and routing information of
the network, the operator knows \emph{exactly} what is going on in the
network, rendering network management tasks relatively easy. If the
traffic matrix is completely unknown, then a network operator is
blind. Subtle faults may plague their network, substantially reducing
performance. Congestion may be rife, or sudden shifts in traffic may
cause transient traffic losses. 

The issues are becoming more important.  The dominant philosophy in
the early days of the Internet was best effort delivery. Most
applications did not have high quality of service (QoS) requirements,
and had some tolerance to packet drops and link failures. In recent
years, however, the landscape of the Internet is changing quickly with
the introduction of streaming content such as video, high definition
television and Voice over IP (VoIP), which have more stringent QoS
requirements. For example, excessive packet drops and delays would
produce highly noticeable artefacts in streamed video. These changes
are driven primarily by user demands, with the introduction of new
applications, such as online social networking, entertainment services
and online multi-player gaming. Furthermore, with the increasing
computational power of mobile devices and increasing wireless speeds,
it is evident that a significant portion of future traffic will be
generated by these devices. These trends are making measurements more
and more critical.

For most operators, the state of their measurements is somewhere
between extremes. Most operators have some traffic data (if only link
counts). However, it is rare (in our experience) to find a network
operator who knows everything about their traffic. As such, one of the
tasks we shall consider here is how to measure these
matrices\footnote{This chapter is not really a primer on measurement
  tools as such, so much as the principles that underlie those
  tools. We will not tell the reader how to set up NetFlow on the reader's particular
  router, but instead we will aim to inform the reader what could be achieved
  with this type of flow-level measurements.}, but also how to obtain
estimates of traffic matrices when presented with incomplete data. The
traffic matrix {\em inference} or {\em completion} or {\em recovery}
problem is one of the major areas of research into these interesting
creatures, and it is intricately related to modelling the matrices,
both as measurements supply data to populate the models, and because
models are used to perform the inference.  Even those operators with
extensive measurements and exact knowledge of today's traffic matrix
may be interested in methods to predict their matrices for use in
future planning, and this can be seen as another form of matrix
completion.

Traffic matrices have many uses, apart from the simple fact that this
type of business intelligence is critical to understanding your
network. The more direct uses include network optimisation, anomaly
detection and protocol design.

There are three common optimisation problems on networks.  Capacity
planning is needed to ensure there is adequate bandwidth for users in
the present and future, but at minimal cost. There are two types of
network planning: evolutionary planning and green fields planning; see
\autoref{sec:applications} for details. Traffic engineering tasks
include day-to-day maintenance of the network as well as predicting
growth trends and anticipating traffic demands \cite{Mitra05StochasticTE,Buriol03GAOSPF,Murphy02TE,Fortz02OSPF,Fortz03OSPF,Nucci07IGP,Uhlig04Implications,Feamster03BGPTE,Rexford06Route}. Routing involves
organising traffic flow in the network, as well as modelling routing of 
large networks \cite{Quoitin05CBGP}. This includes functions such
as finding the shortest paths for flows but also, importantly, load
balancing to ensure links remain uncongested. In all these cases, the
traffic matrix is a key input to perform the tasks effectively and
efficiently.

Traffic matrices can also be used to detect sudden shifts in traffic
due to anomalies. Anomalies include sudden unexpected events, such as
network failures, or more malicious events, such as the September 11
World Trade Centre attack, worm infections and distributed denial of
service (DDoS) attacks \cite{Roughan02BRvariable}. Regardless, these
anomalies need to be detected so as to develop appropriate measures
against possible threats to the network. 

Traffic matrices may also be used to conduct reliability analyses,
where the effect (on traffic) of network failures is considered. A
basic task in most network design is to create redundant paths to
carry traffic in case of failure, but if high reliability is required,
then an operator should also ensure that there is sufficient capacity
in the network to carry this traffic along its alternate paths.

Further, the performance of many network protocols depends on the
traffic they carry, and the cross-traffic which forms their
background. Design of new protocols therefore requires realistic
measurements, or models of such traffic.  Models can be used to test
protocols on artificially synthesised traffic. In this way, the
limitations of a protocol may be understood in a controlled
environment before running it on an actual network.

These issues will be examined in-depth in \autoref{sec:applications},
where algorithms utilising traffic matrices to perform these tasks
will be discussed.

%The
%information obtained by the operator is also useful in addressing customer queries on their bandwidth usage, how their
%traffic is routed (as well as related issues on privacy) or the overall quality of the service. Moreover, the network
%operator can use the same information for accounting purposes when billing customers, or modeling and predicting customer
%demands.

%Finally, the network operator requires the traffic matrix to rate the quality of the network. Generally, there is a 
%minimum standard to the network quality in terms of traffic loss, delay and jitter, as outlined in Service Level Agreement
%(SLA). Although more sophisticated SLAs may appear in the future, the traffic volume from a source to its destination is
%ultimately required by the network operator to measure quality.

\subsection{A primer on modelling}

Motivated by a lack of understanding of Internet traffic, and as a
response to the recent shifting landscape of traffic demands, various
traffic models have been developed over the last decade or so. There
are hundreds of papers describing data traffic modelling, however,
here we shall focus on one group of such models that are particularly
useful to operators: models for traffic matrices. There is no single
model that captures all observed properties of once and future traffic
matrices, and so here we examine the issues that go into deciding on
appropriate models for particular tasks.

% On one hand, the rapidly changing state of the Internet strongly
% motivates the collection of data and development of models of the
% traffic matrices of the networks that serve as the lynchpins of the
% Internet. 

The ultimate goal is to use these models in various networking
tasks, and the ``essential'' qualities that the model must capture
depend critically on the task. For instance, traffic properties are
highly dependent on time scale: in very short time scales (seconds), 
the traffic volume distributions have been shown to exhibit
complex statistical behaviour such as high variability, long-range
dependence, and multi-fractal behaviour. On long time scales (hours to
days to weeks) traffic exhibits strong periodicities: there are
distinct diurnal and weekly cycles with traffic peaks occurring around
mid-day or in the evening and troughs in the early morning.  This
pattern correlates to user behaviour, where they access the Internet
during the day for work or school and sleep in the night. There is
also an observable long-term growth trend underlying the traffic
volume when measured over years, corresponding to increasing global
traffic demand. However, most traffic matrices are measured at some
intermediate time scale, often at 5 to 15 minute intervals.

The time scale is not just a property of the measurements though. The
tasks we wish to perform usually have an associated time scale as
well. In capacity planning, we are often concerned with a ``busy
hour'' -- certainly peak measures over some intermediate period are
important. However, anomaly detection needs to act at fine time
scales: minutes, or perhaps even seconds.

We also have to consider the {\em planning horizon} of a
task. Capacity planning may be aimed at determining the correct
network design six months in advance or more, whereas traffic
engineering is sometimes conducted on scales as short as a day. The
planning horizon determines how far in advance we must predict traffic
matrices. 

% to predict future
% traffic matrices, and their impact on a network topology. The other
% significant gain is the ability to simulate traffic matrices for the
% purposes of testing traffic predictions, anomaly detection and routing
% weight changes. Considering the value of knowing the traffic matrix of
% a network, clearly, having a model that sufficiently describes the
% dynamics of the matrix, while balancing the number of parameters it
% has to enable tractable analysis and simulation, will be an important
% traffic engineering tool.

We can start to see that modelling traffic matrices is a challenging
task. Moreover, there are complexities layered upon complexities. The
Internet is designed in terms of layers, with different protocols
overlaid atop each other. Such a paradigm was adopted to ensure that
each layer works independently, in theory, of each other, although
there are some overlaps between these layers in practice.  The basic
properties of traffic matrices will depend on the network level, and
traffic can be measured between logical or physical
source/destinations, or at different levels of aggregation.

Measurements of networks clearly serve as the foundation in any model
development. The caveat, however, is that the measurements themselves
are subject to errors and inconsistency which may lead to an incorrect
model. Moreover, several hypotheses may fit a particular observation
of the network, leading to several possible models explaining the same
observation. To argue for the use of one model over another requires
additional knowledge from new data or from domain knowledge. And when
new information become available, there is a question of how to
incorporate it into the model. There are a variety of possible
approaches, all equally valid \cite{Alderson06Topology}.

To further compound the problem, underlying all Internet traffic is 
the fact that all traffic is driven by consumer demands. Unfortunately, 
human behaviour is inherently difficult to model let
alone understand. Furthermore, changing trends in network usage,
deployment of Content Distribution Networks (CDNs), and increasing mobile traffic
all have implications on traffic measurement. Although measurements could be
made extremely accurate, if one overlooks the costs involved, 
the observations themselves may be outdated very quickly. 
Therefore, a complicated model based on these measurements
may be accurate for today, but
fail in predicting traffic demands for the next few years or so, given
fluctuations in demand and unexpected changes in traffic patterns
\cite{Teixeira04PotatoSIG}.  

%We can now see that a good model should capture the 
%essential characteristics of the underlying
%traffic, while being robust enough to cope with changes in traffic,
%for example, with regards to the time of the day, or the introduction
%of unexpected events, such as attacks on the network. Much can be
%gained from a good model, enabling the categorisation and analysis of
%the data in a systematic, and desirably, a simplified manner. 

Furthermore, it is misleading to talk about a ``correct'' traffic
matrix model. As pointed out in \cite{Alderson06Topology}, just
because a model replicates some set of properties of the observed data
does not necessarily mean the model is ``correct''. At the least,
there is the dangerous possibility of over-fitting. After all, a
better fit is always achievable simply by adding more parameters to
the model.  Several information criteria address the over-fitting
problem, for instance the Akaike information criterion (AIC)
\cite{Akaike74AIC}, Bayesian information criterion (BIC)
\cite{Schwarz78BIC}, Minimum Message Length (MML) \cite{Wallace99MML}
or the Minimum Description Length (MDL) \cite{Rissanen83MDL}. While
these criteria are beyond the scope of our discussion, the basic
principle is to choose the simplest explanation (measured in some
information metric) amongst all competing explanations of the data.

% The issue highlights the
% difficulty of choosing the number of parameters in a model, as it
% requires several tradeoffs between simplicity and its connection to
% reality.

It is for these reasons models should be evaluated not just on their
accuracy in making predictions of particular statistics, but also
their simplicity, robustness and consistency in relation to the
realities of network operations. Model assumptions should ``make
sense'' to an operator as well as be empirically tested on various
datasets to understand their reliability and pitfalls, which is not to
say we cannot learn new ideas and principles from measurements. We just
need to keep in mind their scope of application, and the usefulness of
these principles in practice may be limited.  It is often preferable to have simple, robust models, in
preference to precise, but fragile ones.

At a fundamental level we need to accept that models are all
unrealistic in some way. A model describing the properties of a
smaller scale network, such as a Local Area Network (LAN), may be
unsuitable at the backbone network level. The underlying assumptions
of one may not hold in the other. Models are simplifications of the
glorious complexity that comprises humanity's primary means of
telecommunication. We must, instead, reread the adage, by George
E.~P.~Box: ``All models are wrong, but some are useful''.  Some models
have been more successfully used in real networks, and it is to these
that we shall devote the most time here. However, we shall endeavour to
cover the majority of simple models with the view that individuals
should use the best model {\em for their application} without fear or
favour.

No doubt, the murkiness and apparent self-contradictions of this
discussion have left our readers no wiser, as yet. Modelling is a
topic that could be discussed endlessly. It is our aim that through
consideration of the qualities of various traffic matrix models, we
shall not only inform about these particular models, but also bring
the reader to a new understanding of modelling to make these
issues a little less opaque.


%   The important principle is to know why they
% work, hopefully shedding light on the variability of traffic flow and
% properties.

\subsection{Chapter outline}

The theme of this chapter is to directly report on all key works in
the field. No attempt will be made to provide overt commentary on what
techniques or models are good or bad, as the objective of this chapter
is to present comprehensive summaries of existing work. It is
important to note that the techniques, algorithms and models presented
here were developed as tools to suit specific applications. Hence, it
is the onus of the practitioner to evaluate and decide which of these
are applicable to his or her situation. Whenever possible, the
strengths and weaknesses of the inference techniques and models are
discussed objectively, so as to minimise the problem of a practitioner
treating a particular tool as the silver hammer for all proverbial
nails.

The chapter is organised as follows. In \autoref{sec:tm}, an overview
is provided on traffic matrices: the basic definitions and some
illustrations to give a better handle on the topic.
\autoref{sec:measurements} discusses how data on traffic matrices are
collected in practice. \autoref{sec:models} discusses the various
models proposed in literature that aims to capture the statistical
properties of traffic matrices. \autoref{sec:applications} goes into a
more in-depth treatment of the applications of traffic matrices, in
particular, how traffic matrix models are used for inference, network
optimisation applications, anomaly detection and traffic matrix
synthesis. Not everything is known about these matrices, and
\autoref{sec:future} summarises some open questions 
and concludes by giving some thoughts on what the future holds for
traffic matrix research. The chapter is concluded in \autoref{sec:conclusion}.

